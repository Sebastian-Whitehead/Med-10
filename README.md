# Med-10 Project

### Overview
The **Med-10** project is developed in connection with a Medialogy master thesis project. It aims to test a pretrained model's ability to assess the quality of synthetic data generated in Unity using performance metrics. This project serves as a baseline implementation for evaluating the performance of models when exposed to synthetic data, without needing dedicated systems for quality assessment. The core hypothesis is that the modelâ€™s performance on new synthetic data correlates with the similarity to data it was trained on, implying that high performance on synthetic data indicates good data quality.

### Project Structure

This repository consists of two primary components:

1. **Data Synthesizer** (Unity-based):  
   The data synthesizer allows for the degradation of specific parameters such as lighting, polycount (via decimation), and texture quality (via mipmaps). This enables experimentation with various data qualities to analyze their impact on model performance.

2. **Testing Program** (Python-based):  
   The testing program loads the synthetic data and evaluates it using a **YOLOv11 model** pretrained on a real-world dataset. It allows for batch testing of one or more "test sets" against each other to compare performance using key metrics like **mAP**, **IoU**, **precision**, **recall**, and **standard deviation**.

### Metrics
The following metrics are used to evaluate the model's performance on synthetic data:
- **Mean Average Precision (mAP)**
- **Intersection over Union (IoU)**
- **Precision**
- **Recall**
- **Standard Deviation (std)**

### Unity Environment (`Final-Unity-Environment` branch)
The Unity environment in the `Final-Unity-Environment` branch simulates a controlled environment where synthetic data is generated. This data is then used to evaluate the performance of the vision model in the testing program.

- **Lighting**: Controlled to vary across test sets.
- **Polycount**: Models are decimated to different levels, allowing the analysis of how polycount impacts data quality.
- **Texture Quality**: Managed through Unity's mipmap system.

### Vision System (`vision` branch)
The vision system relies on a self-trained **YOLOv11** model, which uses a dataset originally used to train a **YOLOv8** model. Since the original YOLOv8 model weights were inaccessible, the YOLOv11 model was trained using the same dataset to replicate the behavior and performance of the original system.

### Data Generation and Testing Workflow

1. **Data Synthesizer (Unity)**:  
   The Unity environment is used to generate synthetic data with varying lighting, polycount, and texture quality.
   
2. **Data Testing (Python)**:  
   The testing program uses a YOLOv11 model pretrained on real data to evaluate the synthetic test sets, comparing their performance using the defined metrics.

**Diagram: Unity Data Generation Flow**  
![Unity Flow](https://github.com/Sebastian-Whitehead/Med-10/blob/main/Data%20generator%20flow%20diagram.jpg?raw=true)

**Diagram: Testing Program Flow**  

![Testing Flow](https://github.com/Sebastian-Whitehead/Med-10/blob/main/Testing%20program%20flow%20diagram%20.jpg?raw=true)


### Setup

#### Dependencies
- **Unity**: Required to run the Unity environment and generate synthetic data.  
- **Python 3.12**: For running the testing program.  
  - Required libraries: `torch`, `opencv`, `numpy`, etc. (List the exact versions if needed)

#### Running the Project

1. **Clone the repository**:  
   ```bash
   git clone https://github.com/Sebastian-Whitehead/Med-10.git
2. Set up Unity:
- Open the Unity project in Unity Editor.
- Install dependencies: Unity Perception Package & UnityMeshSimplifier
- Adjust the program:
  - Control the degradation paramters throguh the "Graphics levels" object in the inspector
  - Define a maximum number of frames within the "Testing Assembly" > "Obj Spawn Area" gameobject 
  - Press play, wait a few seconds, press space to star the generation process
  - The program will automatically stop when it has reached the frame limit
- Access Generated Data in Solo format from "C:/Users/<username>/AppData/LocalLow/DefaultCompany/Ray Tracing Sample Scene/solo_n"

3. Convert the synthetic data:
This can be done through the use of the patch version of PySoloTools. See [GitHub - pysolo2yolo](https://github.com/Sebastian-Whitehead/pysolo2yolo)

4. Run the Testing Program:
- Prepare the synthetic data generated by Unity. 

- Use the provided Python scripts to evaluate the model's performance on this data.

### Customization

Since this is a proof-of-concept baseline, you are free to customize the Unity data generation pipeline and the Python testing program for your own experiments. In the current implementation you can adjust lighting, polycount, texture quality, or add new testing metrics. Note you are not required to use the provided data generator with the testing program, though little modification to the systems any Yolo Formatted dataset can be passed through the testing program with which ever model one chooses. 

### Data and Results

You can choose to use the pregenerated datasets through:[[Hugging Face Dataset](https://huggingface.co/datasets/P4rz1val/SyntheticBeverages)].  Or you can generate your own data through the generation tool provided.
